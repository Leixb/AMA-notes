
@book{wand_kernel_1994,
	title = {Kernel Smoothing},
	isbn = {978-0-412-55270-0},
	abstract = {Kernel smoothing refers to a general methodology for recovery of underlying structure in data sets. The basic principle is that local averaging or smoothing is performed with respect to a kernel function.This book provides uninitiated readers with a feeling for the principles, applications, and analysis of kernel smoothers. This is facilitated by the authors' focus on the simplest settings, namely density estimation and nonparametric regression. They pay particular attention to the problem of choosing the smoothing parameter of a kernel smoother, and also treat the multivariate case in detail. Kernal Smoothing is self-contained and assumes only a basic knowledge of statistics, calculus, and matrix algebra. It is an invaluable introduction to the main ideas of kernel estimation for students and researchers from other discipline and provides a comprehensive reference for those familiar with the topic.},
	pagetotal = {230},
	publisher = {{CRC} Press},
	author = {Wand, M. P. and Jones, M. C.},
	date = {1994-12-01},
	langid = {english},
	note = {Google-Books-{ID}: {GTOOi}5yE008C},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@book{chacon_multivariate_2018,
	location = {New York},
	title = {Multivariate Kernel Smoothing and Its Applications},
	isbn = {978-0-429-48557-2},
	abstract = {Kernel smoothing has greatly evolved since its inception to become an essential methodology in the data science tool kit for the 21st century. Its widespread adoption is due to its fundamental role for multivariate exploratory data analysis, as well as the crucial role it plays in composite solutions to complex data challenges. 
Multivariate Kernel Smoothing and Its Applications offers a comprehensive overview of both aspects. It begins with a thorough exposition of the approaches to achieve the two basic goals of estimating probability density functions and their derivatives. The focus then turns to the applications of these approaches to more complex data analysis goals, many with a geometric/topological flavour, such as level set estimation, clustering (unsupervised learning), principal curves, and feature significance. Other topics, while not direct applications of density (derivative) estimation but sharing many commonalities with the previous settings, include classification (supervised learning), nearest neighbour estimation, and deconvolution for data observed with error.
For a data scientist, each chapter contains illustrative Open data examples that are analysed by the most appropriate kernel smoothing method. The emphasis is always placed on an intuitive understanding of the data provided by the accompanying statistical visualisations. For a reader wishing to investigate further the details of their underlying statistical reasoning, a graduated exposition to a unified theoretical framework is provided. The algorithms for efficient software implementation are also discussed.
José E. Chacón is an associate professor at the Department of Mathematics of the Universidad de Extremadura in Spain.Tarn Duong is a Senior Data Scientist for a start-up which provides short distance carpooling services in France. Both authors have made important contributions to kernel smoothing research over the last couple of decades.},
	pagetotal = {248},
	publisher = {Chapman and Hall/{CRC}},
	author = {Chacón, José E. and Duong, Tarn},
	date = {2018-05-14},
	doi = {10.1201/9780429485572},
}

@book{bowman_applied_1997,
	title = {Applied Smoothing Techniques for Data Analysis: The Kernel Approach with S-Plus Illustrations},
	isbn = {978-0-19-154569-6},
	shorttitle = {Applied Smoothing Techniques for Data Analysis},
	abstract = {The book describes the use of smoothing techniques in statistics, including both density estimation and nonparametric regression. Considerable advances in research in this area have been made in recent years. The aim of this text is to describe a variety of ways in which these methods can be applied to practical problems in statistics. The role of smoothing techniques in exploring data graphically is emphasised, but the use of nonparametric curves in drawing conclusions from data, as an extension of more standard parametric models, is also a major focus of the book. Examples are drawn from a wide range of applications. The book is intended for those who seek an introduction to the area, with an emphasis on applications rather than on detailed theory. It is therefore expected that the book will benefit those attending courses at an advanced undergraduate, or postgraduate, level, as well as researchers, both from statistics and from other disciplines, who wish to learn about and apply these techniques in practical data analysis. The text makes extensive reference to S-Plus, as a computing environment in which examples can be explored. S-Plus functions and example scripts are provided to implement many of the techniques described. These parts are, however, clearly separate from the main body of text, and can therefore easily be skipped by readers not interested in S-Plus.},
	pagetotal = {205},
	publisher = {{OUP} Oxford},
	author = {Bowman, Adrian W. and Azzalini, Adelchi},
	date = {1997-08-14},
	langid = {english},
	note = {Google-Books-{ID}: 7WBMrZ9umRYC},
	keywords = {Mathematics / Probability \& Statistics / General},
}

@book{hastie_elements_2009,
	location = {New York, {NY}},
	title = {The Elements of Statistical Learning},
	isbn = {978-0-387-84857-0 978-0-387-84858-7},
	url = {http://link.springer.com/10.1007/978-0-387-84858-7},
	series = {Springer Series in Statistics},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	urldate = {2022-11-12},
	date = {2009},
	doi = {10.1007/978-0-387-84858-7},
	keywords = {Averaging, Boosting, classification, clustering, data mining, machine learning, Projection pursuit, Random Forest, supervised learning, Support Vector Machine, unsupervised learning},
	file = {Full Text PDF:/home/leix/Documents/Zotero/storage/NGHNIJUZ/Hastie et al. - 2009 - The Elements of Statistical Learning.pdf:application/pdf},
}

@book{fan_statistical_2020,
	location = {New York},
	title = {Statistical Foundations of Data Science},
	isbn = {978-0-429-09628-0},
	abstract = {Statistical Foundations of Data Science gives a thorough introduction to commonly used statistical models, contemporary statistical machine learning techniques and algorithms, along with their mathematical insights and statistical theories. It aims to serve as a graduate-level textbook and a research monograph on high-dimensional statistics, sparsity and covariance learning, machine learning, and statistical inference. It includes ample exercises that involve both theoretical studies as well as empirical applications.
The book begins with an introduction to the stylized features of big data and their impacts on statistical analysis. It then introduces multiple linear regression and expands the techniques of model building via nonparametric regression and kernel tricks. It provides a comprehensive account on sparsity explorations and model selections for multiple regression, generalized linear models, quantile regression, robust regression, hazards regression, among others. High-dimensional inference is also thoroughly addressed and so is feature screening. The book also provides a comprehensive account on high-dimensional covariance estimation, learning latent factors and hidden structures, as well as their applications to statistical estimation, inference, prediction and machine learning problems. It also introduces thoroughly statistical machine learning theory and methods for classification, clustering, and prediction. These include {CART}, random forests, boosting, support vector machines, clustering algorithms, sparse {PCA}, and deep learning.},
	pagetotal = {774},
	publisher = {Chapman and Hall/{CRC}},
	author = {Fan, Jianqing and Li, Runze and Zhang, Cun-Hui and Zou, Hui},
	date = {2020-09-21},
	doi = {10.1201/9780429096280},
}

@book{lange_numerical_1999,
	location = {New York},
	title = {Numerical Analysis for Statisticians},
	isbn = {978-0-387-94979-6},
	url = {http://link.springer.com/10.1007/b98850},
	series = {Statistics and Computing},
	publisher = {Springer-Verlag},
	author = {Lange, Kenneth},
	urldate = {2022-11-12},
	date = {1999},
	langid = {english},
	doi = {10.1007/b98850},
	keywords = {algorithms, expectation–maximization algorithm, linear regression, Markov chain Monte Carlo, Newton's method, optimization, {STATISTICA}},
	file = {Full Text PDF:/home/leix/Documents/Zotero/storage/UHMSBWTK/1999 - Numerical Analysis for Statisticians.pdf:application/pdf},
}

@book{wasserman_all_2006,
	location = {New York, {NY}},
	title = {All of Nonparametric Statistics},
	isbn = {978-0-387-25145-5},
	url = {http://link.springer.com/10.1007/0-387-30623-4},
	series = {Springer Texts in Statistics},
	publisher = {Springer},
	author = {Wasserman, Larry},
	urldate = {2022-11-12},
	date = {2006},
	langid = {english},
	doi = {10.1007/0-387-30623-4},
	keywords = {Excel, Parametric statistics, {STATISTICA}, statistics, {WholePage}},
	file = {Full Text PDF:/home/leix/Documents/Zotero/storage/Y4XUQYRS/2006 - All of Nonparametric Statistics.pdf:application/pdf},
}

@inproceedings{ester_density-based_1996,
	location = {Portland, Oregon},
	title = {A density-based algorithm for discovering clusters in large spatial databases with noise},
	series = {{KDD}'96},
	abstract = {Clustering algorithms are attractive for the task of class identification in spatial databases. However, the application to large spatial databases rises the following requirements for clustering algorithms: minimal requirements of domain knowledge to determine the input parameters, discovery of clusters with arbitrary shape and good efficiency on large databases. The well-known clustering algorithms offer no solution to the combination of these requirements. In this paper, we present the new clustering algorithm {DBSCAN} relying on a density-based notion of clusters which is designed to discover clusters of arbitrary shape. {DBSCAN} requires only one input parameter and supports the user in determining an appropriate value for it. We performed an experimental evaluation of the effectiveness and efficiency of {DBSCAN} using synthetic data and real data of the {SEQUOIA} 2000 benchmark. The results of our experiments demonstrate that (1) {DBSCAN} is significantly more effective in discovering clusters of arbitrary shape than the well-known algorithm {CLAR}-{ANS}, and that (2) {DBSCAN} outperforms {CLARANS} by a factor of more than 100 in terms of efficiency.},
	pages = {226--231},
	booktitle = {Proceedings of the Second International Conference on Knowledge Discovery and Data Mining},
	publisher = {{AAAI} Press},
	author = {Ester, Martin and Kriegel, Hans-Peter and Sander, Jörg and Xu, Xiaowei},
	urldate = {2022-11-12},
	date = {1996-08-02},
	keywords = {arbitrary shape of clusters, clustering algorithms, efficiency on large spatial databases, handling nlj4-275oise},
}
