%! TEX root = ../000-main.tex
\chapter{Introduction to interpretability in machine learning}

\section{Model interpretability and variable relevance}

In a stimulating and provocative paper, Breiman (2001)
% TODO: cite
shook the
statistical community by making it to be aware that traditional
Statistics was no longer the only way to learn from data:
\begin{itemize}
	\item \emph{Data modelling culture} (traditional statistics):
	      \begin{itemize}
		      \item Linear regression, logistic regression, additive models, etc.
		      \item They allow to interpret how the response variable is associated with
		            the input variables: \iemph{Transparent models}.
	      \end{itemize}
	\item \emph{Algorithmic Modeling Culture} (machine learning):
	      \begin{itemize}
		      \item Decision trees, neural networks, support vector machines, etc.
		      \item They have extremely good predictive accuracy, and they usually outperform
		            in this criterion statistical models.
		      \item However, they have low interpretability: \iemph{Black-box models}.
	      \end{itemize}
\end{itemize}

From that it follows an apparent dichotomy: \iemph{predictive capacity} versus \iemph{interpretability}.

Breiman claimed for procedures allowing better interpretation of the algorithmic models
results, without giving up their predictive ability.

Machine learning community has been worried about interpretability:
\begin{quote}
	If the users do not trust a model or a prediction, they will not use it.

	\emph{Ribeiro, Singh and Guestrin (2016)}
\end{quote}

In 2018, the General Data Protection Regulation (GDPR) of the European Union
established users' right to explanation: ``When an algorithmic decision
significantly affects a user, he or she has the right to ask for an explanation
of such decision''.

A powerful research line has been developed:
\iemph{Interpretable Machine Learning} (\iemph{IML}) and
\iemph{eXplainable Artificial Intelligence} (\iemph{XAI}).

There are several review papers on this topic.
Barredo-Arrieta et al. (2020) is one of the most
recent and extensive ones. % TODO: cite properly
Also three monographs: Molnar (2019), Biecek and Burzykowski (2021), and
Masís (2021).

There are also many functions and packages in both R and Python.

\section{IML/XAI concepts}

\begin{note}
	The \emph{accuracy} of predictions is no longer the only criterion to evaluate
	the quality of a prediction algorithm.
\end{note}

Desirable properties for predictive models:
\begin{itemize}
	\item \iemph{transparency}
	\item \iemph{interpretability}
	\item \iemph{explainability}
\end{itemize}

However, these concepts are not well defined and are difficult to measure:
\begin{itemize}
	\item Lipton 2018: ``The term \emph{interpretability} is ill-defined''
	\item Barredo-Arrieta et al. 2020: ``The derivation of general metrics
	      to assess the quality of XAI approaches remain as an open challenge.''
\end{itemize}

To fix ideas, the possibility of obtaining information on the
performance of the algorithm, in both the \emph{global} and \emph{local} senses, is
now appreciated.

\subsection{Global vs. Local interpretability}

\begin{definition}{Global interpretability}{}
	Measures of variable importance or relevance.

	Information about the global performance refers to determining which
	is the role of each explanatory variable in the prediction
	process over the whole support of the explanatory variables.
\end{definition}

\begin{definition}{Local interpretability}{}
	Why the prediction model does a particular prediction for a given individual?

	The aim is to provide a meaningful explanation of why the algorithm returns a
	certain prediction given a particular combination of the predicting variable values.

	\tcblower

	\begin{note}
		The local aspect of interpretability is directly related with the users’
		right to explanation advocated for by, for instance, the EU’s GDPR.
	\end{note}
\end{definition}

\subsection{Transparent models versus ``black box'' models}

\begin{definition}{Transparent models}{}
	or interpretable by design models, or white-boxes or glass-boxes.

	Models, that, by design, have an easily interpretable structure.

	Linear models (LM, and generalized linear models (GLM)),
	generalized additive models (GAM, including additive models),
	classification and regression trees (CART),
	decision trees (DT), k-nearest neighbors (kNN) and
	Bayesian models (including Naïve Bayes prediction rules).

	They offer sufficient interpretation and/or diagnostic tools, both numeric and graphic
\end{definition}

\begin{definition}{Black-box models}{}
	or non-transparent models.

	Models, that do not provide a directly interpretable structure.

	These models require additional interpretation tools.

	All the prediction methods not explicitly mentioned above.
\end{definition}

We can divide non-transparent models into two groups:
\begin{itemize}
	\item Models for which there exist a \emph{model-specific} methods
	      for knowledge extraction. These methods require full access to the model
	      structure.
	\item The rest of the models. We can only use \emph{model-agnostic} methods
	      for interpretation.
\end{itemize}

\begin{example}{Models with model-specific tools}{}
	\begin{itemize}
		\item Tree ensembles (including random forests and boosted methods).
		\item Neural networks (NN, includidng deep learning based on
		      multi-layer, recurrent or convolutional NN).
		\item Support vector machines (SVM).
	\end{itemize}
\end{example}

\begin{example}{Characteristics of model-agnostic methods}{}
	\begin{itemize}
		\item Do not need to know the internal structure of the prediction
		      model to be explained.
		\item Only requirements: the ability to evaluate the prediction model
		      repeated times on data from the training or the
		      test set, or on perturbations of them.
		\item They can be applied to any predictive model, even to those
		      having model-specific methods or those that are
		      transparent models.
	\end{itemize}
\end{example}

All the interpretation methods applicable to non-transparent prediction
models are  globally known as \iemph{post-hoc interpretation methods},
a term that encompasses model-specific and model-agnostic methods.

The results provided by these methods can be numerical and graphical,
although most of the methods chose one or the other.

Finally, it is worth mentioning that most of the interpretation
methods are heuristic, and only some of them are derived from
a formal axiomatic statement.

\begin{recap}{Classification of models and interpretability tools}{}

	\begin{tikzpicture}[
			every node/.style={minimum width=2cm, minimum height=1cm},
			node distance=0.25cm,
		]
		\node[draw,text width=6cm] (global) {
			\textbf{Global measures}
			\begin{itemize}
				\item Variable importance by
				      \begin{itemize}
					      \item Leave-one-covariate-out (LOCO)
					      \item Perturbing a variable in the
					            test set: Random permutations, knockoffs,
					            ghost-variables, \dots
				      \end{itemize}
				\item Variable importance based on Sapley's value
				\item Partial dependence plots (PDP)
				\item Accumulated local effects (ALE)
			\end{itemize}
		};

		\node[draw,text width=6cm, right=of global] (local) {
			\textbf{Local measures}
			\begin{itemize}
				\item Local interpretable model-agnostic explanations (LIME)
				\item Local variable importance based on Shapley's value
				\item SHAP (SHapley Additive exPlanations)
				\item Break-down plots
				\item Individual conditional expectation (ICE) plot or
				      ceteris paribus (CP) plot.
			\end{itemize}
		};

		\node[above=0.1cm and 0.1cm of global] (model-agnostic-text) {
			\textbf{Model-agnostic methods}
		};
		\node[draw,fit=(global) (local)(model-agnostic-text)] (model-agnostic) { };

		\node[draw,text width=6cm, above=of model-agnostic] (model-specific) {
			\textbf{Model-specific methods}
			\begin{itemize}
				\item Tree ensembles
				\item NN
				\item SVM
			\end{itemize}
		};


		\node[above=0.1cm and 0.1cm of model-specific] (non-transparent-text) {
			\textbf{Black-boxes: Post-modeling interpretability}
		};
		\node[draw,fit=(non-transparent-text)(model-agnostic)(model-specific)] (non-transparent) { };

		\node[draw,text width=10cm, above=of non-transparent] (transparent) {
			\textbf{Transparent models}
			\begin{multicols}{2}
				\begin{itemize}
					\item Linear model (LM)
					\item GLM
					\item GAM
					\item CART
					\item Rule-based models
					\item Naïve Bayes
					\item kNN
				\end{itemize}
			\end{multicols}
		};

	\end{tikzpicture}
\end{recap}
