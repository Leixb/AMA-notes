%! TEX root = ../000-main.tex
\chapter{Model-agnostic interpretability methods}
\chaptermark{IML agnostic}

\section{Introduction}

Model-agnostic interpretation methods are those that only require
the evaluation of the fitted prediction model on the training set, on
the test set, or on perturbations of them.

No other information from the kind of model at hand is needed.

To be more formal, let $f$ be the prediction function estimated from a
training sample using a generic prediction model.

We assume that $f$ depends on $p$ arguments, so the predicted value
for $x = (x_1, \dots , x_p)$ is $f(x)$.

The only connection between the model-agnostic interpretation
methods and the prediction model is through the function $f$ and,
more specifically, only evaluations of $f$ at different points $x$ are
allowed.

Under this setting, to interpret the prediction model equals to
interpret the prediction function $f$.

This task is essentially the same we would have to do if we wanted
to explore a generic mathematical function $g$ depending on $p$
variables, from which only evaluations are allowed.

Therefore, any procedure that allows exploring a generic function $g$
(using only evaluations of $g$) can be considered a model-agnostic
method that could be used for interpreting a prediction function $f$.

For instance, computing a numerical approximation to the gradient
of $f$ at a point $x$ can be considered a model-agnostic interpretation
method, as well as using this approximation to compute the first
order Taylor expansion of $f$ around $x$.

Model-agnostic methods are specially useful for interpreting
prediction models for which there are no specific interpretation
methods.

Nevertheless, model-agnostic methods can also improve the
interpretation of models that are usually considered interpretable.

Even multiple linear regression could benefit from the application of
some model-agnostic methods.

When a new model-agnostic interpretation method is introduced, a
good practice is to check what it provides when applied to a classical
simple prediction model, as linear regression, logistic regression or
their additive extensions.

In these simple cases, sometimes it is possible to obtain the closed
expression of the new method results and then to relate them to the
standard outputs of the classical methods.

This way the new interpretation method will be either reinforced
(when its classical counterpart is a sensible measure) or called into
question (when the opposite happens).

\section{Global measures of variable relevance}

Let us consider the prediction problem involving the random
vector $(X,\,Z,\,Y),\;X\in\mathds{R}^p,\,Z\in\mathds{R},\,Y\in\mathds{R}$,
where $Y$ is the response variable that should be predicted from $(X,\,Z)$.

A \iemph{prediction function} $f : \mathds{R}^{p+1} \to \mathds{R}$ has
\emph{expected loss} (or \emph{risk}):
\begin{equation*}
	R(f(X,\,Z),\,Y) = \mathbb{E} \left[ L(f(X,\,Z),\,Y) \right]
\end{equation*}
where $L : \mathds{R}\times\mathds{R} \to \mathds{R}^+$ is a \iemph{loss function}
measuring the \emph{cost} associated with predicting $Y$ by $f(X,\,Z)$.

For instance, \iemph{quadratic loss} is defined as:
\begin{equation*}
	L(y,\,f(x,\,z)) = (y - f(x,\,z))^2
\end{equation*}

We consider the problem of measuring the effect of the single
variable $Z$ on the prediction function $f$ when predicting $Y$
by $f(X,\,Z)$. We call that effect the \iemph{variable relevance}
or \iemph{variable importance} of $Z$.

We assume that a training sample of size $n_1$ and
a test sample of size $n_2$ are available.

\subsection{Leave-one-covariate-out (LOCO)}

A simple approach to define the importance of the variable $Z$:
\begin{enumerate}
	\item Fit the model including both $X$ and $Z$.
	\item Fit the model including only $X$ (leave $Z$ out).
	\item The relevance of $Z$ by \iemph{LOCO} is given by the relative
	      decrease in the prediction accuracy when $Z$ is omitted from the model.
\end{enumerate}

This approach is used, for instance, in multiple linear regression
to decide if a variable should be included in the model or not.

\begin{note}
	The model must be fitted \emph{twice}
\end{note}

\subsubsection{Relevance by LOCO at a populational level}

It could happen that there would exists a natural reduced version of
$f$, say $f_p$, depending only on $p$ variables such that $f_p(X)$ would be the prediction
of $Y$ when $Z$ is not available.

For instance, the natural reduced version of $f(X,\,Z) = \beta_0 + X^T\beta_x + Z\beta_z$
could be $f_p(X) = \beta'_0 + X^T\beta'_x$ for some $\beta'_0,\,\beta'_x$ possibly
different from $\beta_0,\,\beta_x$.

In this case, the usual relevance measure of $Z$ is:
\begin{equation*}
	R(f_p(X),\,Y) - R(f(X,\,Z),\,Y) = \mathds{E} \left[ (Y - f_p(X))^2 - (Y - f(X,\,Z))^2 \right]
\end{equation*}
which corresponds to the reduction in the risk function when using $Z$.

An alternative measure of the relevance of $Z$ is:
\begin{equation*}
	\mathds{E}(L(f(X,\,Z),\,Y),f_p(X))) = \mathds{E} \left[ (f_p(X) - f(X,\,Z))^2 \right]
\end{equation*}

Both measures coincide under quadratic loss, when $(Y - f(X,\,Z))$ has zero mean
and it is independent of $(X,\,Z)$.

\begin{example}{Additive model}{}
	\begin{align*}
		Y                                   & = \beta_0 + s_1(X) + s_2(Z) + \varepsilon      \\[0.5em]
		f(X,\,Z) = \mathds{E}(Y \mid X,\,Z) & = \beta_0 + s_1(X) + s_2(Z)                    \\
		f_p(X) = \mathds{E}(Y \mid X)       & = \beta_0 + s_1(X) + \mathds{E}(s_2(Z) \mid X)
		= \beta_0 + s_1'(X)
	\end{align*}

	The relevance of $Z$ by LOCO is:
	\begin{equation*}
		\mathds{E}(L(f(X,\,Z),\,Y),f_p(X))) = \mathds{E} \left[ (s_2(Z) - \mathds{E}(s_2(Z) \mid X))^2 \right]
		= \mathds{E} \left[ \text{Var}(s_2(Z) \mid X)\right]
	\end{equation*}

	\tcbline

	Under additional linearity: $Y = \beta_0 + X^T\beta_X + Z\beta_Z + \varepsilon$,
	the relevance of $Z$ by LOCO is:
	\begin{equation*}
		\mathds{E}\left[ \text{Var}(Z \beta_Z \mid X)\right] = \beta_Z^2 \mathds{E}\left[ \text{Var}(Z \mid X)\right]
	\end{equation*}
\end{example}


\subsection{Variable importance by random permutations}

% TODO: cite
In the context of random forests (RF), Breiman (2001) proposed an
alternative to LOCO:
\emph{to randomly permute the values of the variable $Z$ in the test sample}
(the out-of-bag dataset in RF).

\begin{algorithm}{Variable importance by random permutations}{}
	\begin{enumerate}
		\item Fit the model with the training sample using all the original
		      explanatory variables, $X$ and $Z$.
		\item Evaluate the accuracy of the estimated model in the test sample using the observed values of
		      $X$ and $Z$.
		\item Replace the values of $Z$ in the test sample by a random permutation
		      $Z'$.
		\item Evaluate the accuracy of the estimated model in the test sample using the observed values of $X$
		      and the permuted values $Z'$.
		\item The relevance of $Z$ by \emph{random permutations} is given by the relative
		      decrease in the prediction accuracy when $Z$ is replaced by $Z'$.
	\end{enumerate}
	\tcblower
	\begin{note}
		Steps 3 and 4 can be repeated $n$ times and average the accuracy measures in step 5.
	\end{note}
	\begin{note}
		The model is \emph{estimated only once}.
	\end{note}
\end{algorithm}

\subsubsection{Random permutations at a population level}

The population counterpart of taking random permutations of values
of $Z$ in the test sample, is to \emph{replace the random variable $Z$ by and independent
	copy of it, $Z'$}. with the same marginal distribution as
$Z$ but independent from $(X,\,Y)$.

This approach does not require the reduced version $f_p$ of $f$.

In this way, the relevance measure of $Z$ will be:
\begin{equation*}
	\mathds{E}(L(f(X,\,Z),\,Y),f(X,\,Z')) = \mathds{E} \left[ (f(X,\,Z) - f(X,\,Z'))^2 \right]
\end{equation*}

\begin{example}{Additive model}{}
	Consider the case of $f$ being \emph{additive} in $X$ and $Z$:
	\begin{equation*}
		f(X,\,Z) = \beta_0 + s_1(X) + s_2(Z)
	\end{equation*}
	with $\mathds{E}(s_2(Z)) = 0$.

	Under \emph{quadratic loss}:
	\begin{align*}
		\mathds{E}(L(f(X,\,Z),\,Y),f(X,\,Z')) & = \mathds{E} \left[ (f(X,\,Z) - f(X,\,Z'))^2 \right] \\
		                                      & = \mathds{E} \left[
			\left(
			(\beta_0 + s_1(X) + s_2(Z)) - (\beta_0 + s_1(X) + s_2(Z'))
			\right)^2
			)
		\right]                                                                                      \\
		                                      & = 2\text{Var}(s_2(Z))
	\end{align*}

	if additional linearity happens, $s_2(Z) = Z\beta_Z$, then this relevance measure
	equals $2\beta_Z^2 \text{Var}(Z)$.
\end{example}

\subsubsection{Undesirable properties of random permutations}

At first glance, the relevance measures of $2\text{Var}(s_2(Z))$ and $2\beta_Z^2 \text{Var}(Z)$
seem to be suitable, but:
\begin{itemize}
	\item The relevance of $Z$ would be the same in two completely different scenarios:
	      \begin{enumerate}
		      \item $X$ and $Z$ are independent; $Z$ encode exclusive information about $Y$.
		      \item $X$ and $Z$ are strongly related; in such a case $X$ could make up for the
		            absence of $Z$.
	      \end{enumerate}
	      Clearly, $Z$ is more relevant in the first scenario, neither
	      $2\text{Var}(s_2(Z))$ nor $2\beta_Z^2 \text{Var}(Z)$ can distinguish between the two scenarios.
	      \begin{note}
		      This is not the case in relevance by LOCO which
		      is $\beta_Z^2 \mathds{E} \left[ \text{Var}(Z \mid X)\right]$.
	      \end{note}
	\item The replacement of $Z$ by an independent copy $Z'$ implies a drastic alteration
	      of the prediction function $f(X,\,Z)$.
	      \begin{itemize}
		      \item Consider again the simple case of the linear predictor
		            $f(X,\,Z) = \beta_0 + X^T\beta_X + Z\beta_Z$.
		      \item When replacing $Z$ by $Z'$, the prediction function becomes:
		            \begin{equation*}
			            f(X,\,Z') = \beta_0 + X^T\beta_X + Z'\beta_Z =
			            (\beta_0 - \mathds{E}(Z)\beta_Z) + X^T\beta_X + (Z' + \mathds{E}(Z))\beta_Z
		            \end{equation*}
		            which is equivalent to using the reduced version of $f$:
		            \begin{equation*}
			            f_p(X,\,Z') = \beta'_0 + X^T\beta_X + \nu
		            \end{equation*}
		            where $\beta'_0 = \beta_0 - \mathds{E}(Z)\beta_Z$ and $\nu = (Z' + \mathds{E}(Z))\beta_Z$ is
		            a random noise independent from $(X,\,Y)$, that does not help in any way to predict $Y$.
		      \item A preferred alternative would be to use the reduced version of $f$ directly:
		            \begin{equation*}
			            f_p(X,\,Z) = \beta'_0 + X^T\beta_X
		            \end{equation*}
		            which is equivalent to replacing $Z$ by $\mathds{E}(Z)$ and
		            $\beta'_0 = \beta_0 + \mathds{E}(Z)\beta_Z$.
	      \end{itemize}
	\item When $X$ and $Z$ are strongly related, there is a risk of
	      \emph{extrapolation} when evaluating $f(X,\,Z')$. This can happen since
	      the support of
	      $(X,\,Z)$ could be much smaller than the support of $(X,\,Z)$, which
	      is the Cartesian product of the supports $X$ and $Z$.
\end{itemize}

\subsection{Relevance by ghost variables}

We have seen that replacing $Z$ by $\mathds{E}(Z)$ in $f(X,\,Z)$ is
more appropriate than replacing $Z$ by an independent copy $Z'$ (random
permutations).

\begin{note}
	But, even better is to replace $Z$ by $\mathds{E}(Z \mid X)$:
	the best prediction of $Z$ as a function of $X$ according to the
	quadratic loss.
\end{note}

If there is dependence between $X$ and $Z$, then we expect $\left|Z - \mathds{E}(Z \mid X)\right|$
to be less than $\left|Z - \mathds{E}(Z)\right|$, so
$f(X,\,\mathds{E}(Z \mid X))$ should be closer to $f(X,\,Z)$ than
$f(X,\,\mathds{E}(Z))$.

Therefore, when $Z$ is not available, replacing it by $\mathds{E}(Z \mid X)$
allows $X$ to contribute a little bit more in the prediction of $Y$ than
when $Z$ is replaced by $\mathds{E}(Z)$.

The bigger the contribution of $X$ is, the smaller is the relevance of $Z$
in  the prediction of $Y$, measured by the quadratic loss:
\begin{equation*}
	\mathds{E}(L(f(X,\,Z),\,Y),f(X,\,\mathds{E}(Z \mid X))) =
	\mathds{E} \left[ (f(X,\,Z) - f(X,\,\mathds{E}(Z \mid X)))^2 \right]
\end{equation*}

\begin{marker}
	We call \iemph{ghost variable} of $Z$ any estimator of $\mathds{E}(Z \mid X)$.
\end{marker}

\begin{algorithm}{Relevance by ghost variables}{}
	\begin{enumerate}
		\item Fit the model with the training sample using all the original explanatory variables,
		      $X$ and $Z$.
		\item Evaluate the accuracy of the estimated model in the test sample using the observed values
		      of $X$ and $Z$.
		\item Define the \emph{ghost variable} of $Z$ as $\widehat{Z} = \widehat{\mathds{E}(Z \mid X)}$,
		      where the last estimation is done in the test sample.
		\item Evaluate the accuracy of the estimated model in the test sample using
		      the observed values of $X$ and the ghost variable $\widehat{Z}$.
		\item The \emph{Relevance of $Z$ by its ghost variable} is given by the relative
		      decrease in prediction accuracy in the test sample when $Z$ is replaced by
		      its ghost variable $\widehat{Z}$.
	\end{enumerate}
\end{algorithm}

\begin{note}
	The host variables approach to measure the effect of variable $Z$ combines
	the advantages of LOCO and random permutations:
	\begin{enumerate}
		\item The model is estimated only once.
		\item It gives results more similar to LOCO, which are better than those
		      obtained with random permutations when there are dependence among
		      covariates.
	\end{enumerate}
\end{note}

\begin{example}{Additive model}{}
	If $f$ is additive in $X$ and $Z$, under quadratic loss,
	the relevance of $Z$ by its ghost variable is given by:
	\begin{equation*}
		\mathds{E} \left[
			\left( f(X,\,Z) - f(X,\,\mathds{E}(Z \mid X)) \right)^2
			\right] =
		\mathds{E} \left[
			\left( s_2(Z) - s_2(\mathds{E}(Z \mid X)) \right)^2
			\right]
	\end{equation*}
	which does not coincide neither with LOCO nor with random permutations.

	If additionally there is linearity, $s_2(Z) = Z\beta_Z$, it is
	equal to:
	\begin{equation*}
		\beta_Z^2 \mathds{E} \left[
			\left( Z - \mathds{E}(Z \mid X)\right)^2
			\right] =
		\beta_Z^2 \mathds{E} \left[
			\text{Var}(Z \mid X)
			\right]
	\end{equation*}
	which coincides with LOCO in this case.
\end{example}

\begin{recap}{LOCO, random permutation and Ghost variables}{}
	\begin{itemize}
		\item It is desirable that any variable relevance method would
		      measure meaningful quantities when applied to simple models.
		\item We have seen that for \emph{multiple linear regression}
		      models:
		      \begin{itemize}
			      \item \emph{LOCO} and \emph{ghost variables} give
			            approximately the same results.
			      \item The relevance of variable $Z$ by LOCO and ghost
			            variable is proportional to the clasical
			            $F$-statistic used for testing $H_0: \beta_Z = 0$
			            against $H_1: \beta_Z \neq 0$.
			      \item The relevance by random permutations does not
			            reproduce any standard test statistic for the
			            significance of $\beta_Z$.
		      \end{itemize}
		\item We conclude that \emph{measuring variable relevance by ghost
			      variables combines the advantages of the other two methods}:
		      \begin{itemize}
			      \item The model is trained only once.
			      \item In linear regression it reproduces the significance
			            $F$-statistics.
		      \end{itemize}
		\item When we measure the relevance by ghost variable in a
		      predictive model, we are in some way
		      \emph{extending the concept of variable relevance} to
		      that model.
	\end{itemize}
\end{recap}

\subsection{Other relevance measures based on perturbations}
\index{perturbation}

Random permutations and ghost variables methods for computing
relevance of a explanatory variable $Z$ follow a general
scheme:
\begin{quote}
	To replace the values of $Z$ in the test set by ``perturbed'' values
	of them, which are independent of the response variable $Y$, given
	the other explanatory variables $X$.
\end{quote}

Other possibilities of ``perturbation'' of $Z$ have been considered
recently in the literature.

\subsubsection{Knockoffs}

Consider the lasso estimation of a linear model with
reponse variable $Y$ and explanatory variables
$\boldsymbol{X} = (X_1,\ldots,X_p)$ with large $p$.

When testing the significance of a large number of predictors
the problem of controlling the \iemph{false discovery rate} (FDR)
\index{FDR}
arises.

\begin{definition}{Knockoff variables}{} \index{knockoff}
	% TODO cite
	were defined by Barber and Candès (2015) as
	variables unrelated to the response and that jointly
	have the same distribution as the original ones,
	but being \emph{as different as possible} from them.
\end{definition}

Assuming that the explanatory variables $\boldsymbol{X}$ are
random variables with a known joint distribution, then
the set of \iemph{model-X} (MX)
\index{MX}
knockoff variables $\tilde{\boldsymbol{X}} = (\tilde{X}_1,\ldots,\tilde{X}_p)$,
are independents of $Y$ given $\boldsymbol{X}$, and the joint distribution of
\begin{equation*}
	(\boldsymbol{X},\,\tilde{\boldsymbol{X}}) =
	\left( X_1,\ldots,X_p,\,\tilde{X}_1,\ldots,\tilde{X}_p \right)
\end{equation*}
does not change if we interchange any subset of the original variables
and their knockoffs.

The lasso model is fitted with all $X_i$ and $\tilde{X}_i$ as
explanatory variables.

% TODO: continue next slide on knockoffs

\subsubsection{Estimated conditional distribution}

% TODO: cite
In the context of feature selection in complex predictive models,
Tansey et al. (2022) also deal with the problem of working with
an unknown conditional distribution $(Z \mid X = x)$, when
they describe the general \iemph{Holdout Randomized Test} (HRT)
\index{HRT}.

They model the conditional distribution of $(Z \mid X = x)$
as a \emph{mixture of univariate Gaussians}. They fix the
number of components as 5, then there are
$ 5 + 5  + (5 - 1) = 14$ conditional
parameters to estimate as functions of the $p$ values
of $x$.

% TODO: cite
Then, they propose to estimate the conditional distribution
of $(Z \mid X = x)$ following the proposal of Bishop (1996)
on \iemph{mixture density networks}.

This method uses a neural network with 14 neurons in the
output layer (one for each parameter), instead of having
just one output neuron as it happens when the goal is
to estimate expectation $\mathds{E}(Z \mid X = x)$.

\begin{note}
	The estimation of the conditional distribution models
	is a complicated task, requiring considerable computer effort.

	On the contrary, \emph{ghost variables} require only
	to estimate the conditional expectation using the regression model
	preferred by the user.

	If there are many variables, it may be better to use \emph{lasso}
	type estimation.
\end{note}

\begin{recap}{Knockoffs vs. ghost variables}{}
	\begin{itemize}
		\item Using ghost variables or using knockoffs to compute relevance of
		      features are comparable strategies regarding:
		      \begin{itemize}
			      \item the quality of the resulting relevance measures,
			      \item the computational efficiency.
		      \end{itemize}
		\item Both are preferred to other alternatives considered in our simulation
		      study.
		\item When using ghost variables the practitioner has to propose
		      regression models of each explanatory variable over the others, and
		      then fit these models.
		\item This is a routine process which is easily implemented in any standard
		      platform (R or Python, for instance), even if the linearity assumption
		      is not fulfilled by our data.
		\item On the other hand, generating knockoffs variables is difficult even in
		      the most standard settings.
		\item Moreover, when the data are far from well mimicked with Model-X
		      Gaussian knockoffs there is no easy way to generate knockoffs.
		\item Simplicity and flexibility of the ghost variable procedure are a clear
		      advantage with respect to using knockoffs.
	\end{itemize}
\end{recap}

\subsection{The relevance matrix}

We jointly measure the relevance of all the explanatory variables.

Given the random vector $(X,\,Y)$, $X = (X_1,\ldots,X_p) \in \mathds{R}^p$,
$Y \in \mathds{R}$, the prediction of $Y$ from the $p$ components of
$X$ through  the estimation of the regression function
$m(x) = \mathds{E}(Y \mid X = x)$ is considered.


\subsection{Variable relevance measures as Shapley’s values}
\subsection{Global graphical methods}

% \subsection{Importance of variables through disturbances}
% \subsection{Importance based on the Shapley Value}
% \subsection{Partial dependency graph}
% \subsection{Cumulative local effects graphs}

\section{Local methods}

\subsection{LIME: Local interpretable model-agnostic explanations}
\subsection{Local importance based on the Shapley Value}
\subsection{SHAP: SHApley Additive ExPlanations}
\subsection{Break down graphics}
\subsection{ICE: Individual conditional expectation, or ceteris paribus chart}
